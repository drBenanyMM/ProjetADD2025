{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4282e944",
   "metadata": {},
   "source": [
    "# Big Data Course Slides - Updated for 2025\n",
    "\n",
    "## Slide 1: Introduction\n",
    "**Original Content**: Simple \"Introduction\" title.\n",
    "**Updated Content**:  \n",
    "Welcome to the 2025 Big Data Course! This course explores the foundations and modern practices of handling massive datasets using distributed systems, cloud technologies, and parallel programming. Topics include Big Data concepts, distributed file systems (HDFS), and frameworks like Apache Hadoop and Spark, with practical applications in real-world scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 2: Course Objectives\n",
    "**Original Content**: Lists course topics (Purpose, Big Data, Distributed Systems, Parallel Programming).\n",
    "**Updated Content**:  \n",
    "This week, we introduce:  \n",
    "- **Course Goals**: Understand Big Data challenges and solutions.  \n",
    "- **Big Data Fundamentals**: Managing and analyzing massive datasets.  \n",
    "- **Distributed Systems**: Scalable architectures for data storage and processing.  \n",
    "- **Parallel Programming**: Leveraging frameworks like Hadoop and Spark for efficient computation.  \n",
    "\n",
    "**Updates**: Added Spark to reflect its prominence in 2025 alongside Hadoop.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 3: Why Big Data Matters\n",
    "**Original Content**: Placeholder \"Binto\" and \"$=$\".\n",
    "**Updated Content**:  \n",
    "Big Data drives innovation across industries. According to recent industry reports (e.g., LinkedIn 2024), top skills include:  \n",
    "1. **Cloud and Distributed Computing** (e.g., AWS, Azure, Hadoop, Spark).  \n",
    "2. **Data Analytics and Machine Learning** (e.g., Python, R, TensorFlow).  \n",
    "3. **Data Storage and Management** (e.g., SQL, NoSQL, cloud-native databases).  \n",
    "These skills are critical for roles in AI, IoT, and real-time analytics.\n",
    "\n",
    "**Updates**: Replaced vague placeholder with industry-relevant context and updated skills to 2024/2025 standards.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 4: Big Data in Context\n",
    "**Original Content**: Highlights LinkedIn skills and course relevance.\n",
    "**Updated Content**:  \n",
    "Big Data is a cornerstone of modern technology. LinkedIn’s 2024 skills report emphasizes:  \n",
    "1. **Cloud and Distributed Computing**: Tools like Hadoop, Spark, and Kubernetes dominate.  \n",
    "2. **Data Analytics**: Proficiency in Python, R, and ML frameworks is essential.  \n",
    "3. **Storage Systems**: SQL, NoSQL (e.g., MongoDB, Cassandra), and cloud storage (e.g., S3, GCS).  \n",
    "This course equips you with these in-demand skills for 2025.\n",
    "\n",
    "**Updates**: Updated skillset to include modern tools and cloud platforms.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 5: Understanding Data Scales\n",
    "**Original Content**: Table of prefixes (kilo, mega, etc.) with examples.\n",
    "**Updated Content**:  \n",
    "Big Data involves massive scales. Key prefixes:  \n",
    "\n",
    "| Prefix | Factor | Example |\n",
    "|--------|--------|---------|\n",
    "| Kilo (k) | $10^3$ | A text page (~1 KB) |\n",
    "| Mega (M) | $10^6$ | Network transfer speed (~1 MB/s) |\n",
    "| Giga (G) | $10^9$ | Small datasets (~1 GB) |\n",
    "| Tera (T) | $10^{12}$ | Enterprise databases |\n",
    "| Peta (P) | $10^{15}$ | Social media platforms (e.g., Meta, AWS) |\n",
    "| Exa (E) | $10^{18}$ | Global internet data |\n",
    "| Zetta (Z) | $10^{21}$ | Future AI training datasets |\n",
    "\n",
    "**Updates**: Corrected table formatting, added modern examples, and fixed exponent for Giga ($10^9$ instead of $10^7$).\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 6: What is Big Data?\n",
    "**Original Content**: Defines Big Data with examples (Google, Facebook, Amazon).\n",
    "**Updated Content**:  \n",
    "Big Data refers to datasets too large for traditional systems to process efficiently. Examples in 2025:  \n",
    "- **Google**: ~50 EB of indexed web data.  \n",
    "- **Meta**: ~10 EB of user-generated content, with 20 PB/day added.  \n",
    "- **AWS**: Petabytes of cloud-hosted data for enterprises.  \n",
    "- **Scientific Data**: Telescopes (~1 PB/day), CERN (~500 PB stored).  \n",
    "Big Data requires distributed storage and processing due to its volume, velocity, and variety.\n",
    "\n",
    "**Updates**: Updated data volumes to 2025 estimates and emphasized the \"3 Vs\" of Big Data.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 7: Distributed Systems for Big Data\n",
    "**Original Content**: Discusses data distribution, HDFS, and MapReduce.\n",
    "**Updated Content**:  \n",
    "Processing Big Data requires:  \n",
    "- **Distributed Storage**: Data split across thousands of machines in data centers (e.g., HDFS, cloud storage like S3).  \n",
    "- **Specialized Databases**: NoSQL systems (e.g., Cassandra, MongoDB, Elasticsearch).  \n",
    "- **Parallel Processing**: Frameworks like MapReduce (Hadoop) and Spark for scalable computation.  \n",
    "Traditional databases can’t handle petabyte-scale data, necessitating distributed architectures.\n",
    "\n",
    "**Updates**: Added cloud storage and Spark, clarified limitations of traditional databases.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 8: Inside a Data Center\n",
    "**Original Content**: Describes a cluster of 5000 connected computers.\n",
    "**Updated Content**:  \n",
    "Modern data centers host thousands of servers forming clusters:  \n",
    "- **Scale**: 10,000+ servers in hyperscale facilities (e.g., Google, AWS).  \n",
    "- **Connectivity**: High-speed networks (100 Gbps+) for data sharing.  \n",
    "- **Redundancy**: Data replication ensures fault tolerance.  \n",
    "This course focuses on programming for such clusters using Hadoop and cloud-native tools.\n",
    "\n",
    "**Updates**: Scaled up to reflect 2025 data center sizes and included cloud-native tools.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 9: Server Hardware\n",
    "**Original Content**: Describes blade servers and Google’s approach.\n",
    "**Updated Content**:  \n",
    "A typical 2025 server (e.g., rack or blade):  \n",
    "- **Specs**: 8-16 CPU cores, 2-4 TB RAM, 50-100 TB NVMe storage.  \n",
    "- **Cost**: ~$10,000, with prices dropping due to competition.  \n",
    "- **Trends**: Google and AWS use commodity hardware for cost-efficiency, paired with custom accelerators (e.g., TPUs, GPUs).  \n",
    "These servers form the backbone of Big Data processing.\n",
    "\n",
    "**Updates**: Updated hardware specs and costs to 2025 standards, added accelerators.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 10: Connected Machines\n",
    "**Original Content**: Discusses cloud storage and distributed execution.\n",
    "**Updated Content**:  \n",
    "Clusters connect servers to share:  \n",
    "- **Storage**: Distributed file systems (e.g., HDFS, cloud storage like S3) with data replication.  \n",
    "- **Computation**: Programs run across multiple nodes using frameworks like Hadoop, Spark, or Kubernetes.  \n",
    "This course teaches you to develop applications for such distributed environments using Hadoop and modern APIs.\n",
    "\n",
    "**Updates**: Added Spark and Kubernetes, emphasized cloud integration.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 11: Introduction to Hadoop\n",
    "**Original Content**: Repeated \"Hadoop\" text.\n",
    "**Updated Content**:  \n",
    "**Apache Hadoop** is an open-source framework for distributed storage and processing:  \n",
    "- **HDFS**: Hadoop Distributed File System for scalable storage.  \n",
    "- **YARN**: Resource manager for job scheduling.  \n",
    "- **MapReduce**: Programming model for parallel data processing.  \n",
    "- **Ecosystem**: Integrates with Spark, Hive, and cloud platforms (e.g., AWS EMR).  \n",
    "Hadoop remains relevant in 2025 for on-premises and hybrid cloud deployments.\n",
    "\n",
    "**Updates**: Replaced placeholder with a clear Hadoop overview, added ecosystem context.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 12: Hadoop File System (HDFS)\n",
    "**Original Content**: Title only.\n",
    "**Updated Content**:  \n",
    "**HDFS Overview**:  \n",
    "- **Purpose**: Stores massive datasets across multiple machines.  \n",
    "- **Features**:  \n",
    "  - Fault-tolerant through data replication.  \n",
    "  - Scalable to petabytes.  \n",
    "  - Transparent access via a unified file system view.  \n",
    "- **Use Cases**: Data lakes, batch processing, and analytics.\n",
    "\n",
    "**Updates**: Completed the slide with a concise HDFS introduction.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 13: HDFS Features\n",
    "**Original Content**: Describes HDFS as a distributed file system.\n",
    "**Updated Content**:  \n",
    "**HDFS Characteristics**:  \n",
    "- **Tree Structure**: Organizes files and directories like Unix.  \n",
    "- **Transparency**: Hides physical storage locations from users.  \n",
    "- **Replication**: Files are copied (default: 3 replicas) for reliability and parallel access.  \n",
    "- **Scalability**: Handles petabytes across thousands of nodes, integrated with cloud storage in 2025.\n",
    "\n",
    "**Updates**: Clarified features and added cloud integration.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 14: HDFS File Organization\n",
    "**Original Content**: Compares HDFS to Unix, lists directories.\n",
    "**Updated Content**:  \n",
    "**HDFS File Structure**:  \n",
    "- **Root (/) Structure**: Similar to Unix, with directories like `/hbase`, `/tmp`, `/var`.  \n",
    "- **User Space**: `/user/<username>` for personal files, distinct from `/home`.  \n",
    "- **System Directories**: Includes `/user/hive`, `/user/spark`, `/user/history`.  \n",
    "- **Permissions**: Supports owners, groups, and access rights like ext4.\n",
    "\n",
    "**Updates**: Streamlined description, corrected directory names, and clarified user space.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 15: HDFS Commands\n",
    "**Original Content**: Lists HDFS commands with notes on Java latency.\n",
    "**Updated Content**:  \n",
    "**Common HDFS Commands**:  \n",
    "```bash\n",
    "hdfs dfs -help          # Display help\n",
    "hdfs dfs -ls <path>     # List files\n",
    "hdfs dfs -cat <file>    # View file content\n",
    "hdfs dfs -mv <src> <dst> # Move/rename\n",
    "hdfs dfs -cp <src> <dst> # Copy\n",
    "hdfs dfs -mkdir <dir>   # Create directory\n",
    "hdfs dfs -rm -r <dir>   # Remove directory\n",
    "```\n",
    "**Note**: Commands may have slight latency due to Java-based Hadoop internals.\n",
    "\n",
    "**Updates**: Formatted commands clearly, removed outdated flag notes, and retained latency comment.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 16: HDFS File Operations\n",
    "**Original Content**: Shows commands for file transfers.\n",
    "**Updated Content**:  \n",
    "**Transferring Files with HDFS**:  \n",
    "- **Upload**:  \n",
    "  ```bash\n",
    "  hdfs dfs -put <local> <hdfs_path>  # or -copyFromLocal\n",
    "  hdfs dfs -mkdir -p /user/books\n",
    "  hdfs dfs -put dracula /user/books\n",
    "  ```\n",
    "- **Download**:  \n",
    "  ```bash\n",
    "  hdfs dfs -get <hdfs_path> <local>  # or -copyToLocal\n",
    "  hdfs dfs -get /user/books/center_earth\n",
    "  ```\n",
    "\n",
    "**Updates**: Simplified and standardized command examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 17: How HDFS Works\n",
    "**Original Content**: Explains HDFS block structure and replication.\n",
    "**Updated Content**:  \n",
    "**HDFS Mechanics**:  \n",
    "- **Block Size**: Default 256 MB (configurable, e.g., 128 MB in some setups).  \n",
    "- **Replication**: Each block is replicated (default: 3) across different nodes for fault tolerance and parallel access.  \n",
    "- **Distribution**: Blocks of a file are spread across nodes, not necessarily on the same machine.  \n",
    "This ensures scalability and reliability in 2025’s hybrid cloud environments.\n",
    "\n",
    "**Updates**: Updated block size context and added cloud reference.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 18: HDFS Cluster Roles\n",
    "**Original Content**: Describes NameNode, Secondary NameNode, and DataNodes.\n",
    "**Updated Content**:  \n",
    "**HDFS Architecture**:  \n",
    "- **NameNode**: Manages metadata (file names, block locations).  \n",
    "- **Secondary NameNode**: Periodic backups of NameNode metadata.  \n",
    "- **DataNodes**: Store actual data blocks.  \n",
    "- **Clients**: Access points for interacting with the cluster.  \n",
    "In 2025, high-availability setups often replace Secondary NameNode with standby NameNodes.\n",
    "\n",
    "**Updates**: Clarified roles and noted high-availability trends.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 19: HDFS Node Schema\n",
    "**Original Content**: Diagram of NameNode and DataNodes with file blocks.\n",
    "**Updated Content**:  \n",
    "**HDFS Data Distribution**:  \n",
    "- **NameNode**: Stores metadata (e.g., `toto.txt` maps to blocks A, B, C).  \n",
    "- **DataNodes**: Store blocks (e.g., DN1: A, B, C; DN2: A, C, D).  \n",
    "- **Replication**: Each block is replicated across multiple DataNodes for reliability.  \n",
    "Example: `toto.txt` blocks are distributed and replicated across DN1–DN4.\n",
    "\n",
    "**Updates**: Simplified schema explanation, removed redundant DataNode list.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 20: HDFS Reliability\n",
    "**Original Content**: Explains replication and NameNode importance.\n",
    "**Updated Content**:  \n",
    "**HDFS Fault Tolerance**:  \n",
    "- **Replication**: Blocks are copied (default: 3) across DataNodes for redundancy and parallel access.  \n",
    "- **NameNode**: Central metadata store; a failure can disrupt HDFS.  \n",
    "- **Mitigation**: Secondary NameNode or standby NameNodes in high-availability mode ensure continuity.  \n",
    "This is critical for 2025’s always-on data platforms.\n",
    "\n",
    "**Updates**: Streamlined explanation and emphasized high-availability.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 21: High Availability in HDFS\n",
    "**Original Content**: Describes high-availability mode with standby NameNodes.\n",
    "**Updated Content**:  \n",
    "**HDFS High Availability (HA)**:  \n",
    "- **Standby NameNodes**: Two or more NameNodes in hot standby, ready to take over instantly.  \n",
    "- **JournalNodes**: Synchronize metadata updates across NameNodes.  \n",
    "- **Benefits**: Eliminates single point of failure, replaces Secondary NameNode.  \n",
    "HA is standard in 2025 for production-grade Hadoop clusters.\n",
    "\n",
    "**Updates**: Clarified HA mechanics and its prevalence in 2025.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 22: Java API for HDFS\n",
    "**Original Content**: Title only.\n",
    "**Updated Content**:  \n",
    "**HDFS Java API Overview**:  \n",
    "- **Purpose**: Programmatically interact with HDFS for file operations.  \n",
    "- **Key Classes**: `Configuration`, `FileSystem`, `Path`.  \n",
    "- **Use Cases**: Reading, writing, and managing files in HDFS.  \n",
    "Examples follow in subsequent slides, updated for Hadoop 3.x APIs in 2025.\n",
    "\n",
    "**Updates**: Completed the slide with an API introduction.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 23: HDFS File Information\n",
    "**Original Content**: Incomplete Java code for listing file blocks.\n",
    "**Updated Content**:  \n",
    "**Listing HDFS File Blocks (Java)**:  \n",
    "```java\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.*;\n",
    "import java.io.IOException;\n",
    "\n",
    "public class HDFSInfo {\n",
    "    public static void main(String[] args) throws IOException {\n",
    "        Configuration conf = new Configuration();\n",
    "        FileSystem fs = FileSystem.get(conf);\n",
    "        Path path = new Path(\"/user/apitest.txt\");\n",
    "        FileStatus status = fs.getFileStatus(path);\n",
    "        BlockLocation[] blocks = fs.getFileBlockLocations(status, 0, status.getLen());\n",
    "        for (BlockLocation block : blocks) {\n",
    "            System.out.println(block.toString());\n",
    "        }\n",
    "        fs.close();\n",
    "    }\n",
    "}\n",
    "```\n",
    "**Purpose**: Retrieves block locations for a file in HDFS.\n",
    "\n",
    "**Updates**: Completed and corrected the code, updated for Hadoop 3.x compatibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 24: Reading an HDFS File\n",
    "**Original Content**: Incomplete Java code for reading a file.\n",
    "**Updated Content**:  \n",
    "**Reading an HDFS File (Java)**:  \n",
    "```java\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.*;\n",
    "import java.io.BufferedReader;\n",
    "import java.io.InputStreamReader;\n",
    "\n",
    "public class HDFSRead {\n",
    "    public static void main(String[] args) throws IOException {\n",
    "        Configuration conf = new Configuration();\n",
    "        FileSystem fs = FileSystem.get(conf);\n",
    "        Path path = new Path(\"/user/apitest.txt\");\n",
    "        FSDataInputStream in = fs.open(path);\n",
    "        BufferedReader reader = new BufferedReader(new InputStreamReader(in));\n",
    "        String line = reader.readLine();\n",
    "        System.out.println(line);\n",
    "        reader.close();\n",
    "        fs.close();\n",
    "    }\n",
    "}\n",
    "```\n",
    "**Purpose**: Reads and prints the first line of a text file in HDFS.\n",
    "\n",
    "**Updates**: Completed and corrected the code, ensured Hadoop 3.x compatibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 25: Writing an HDFS File\n",
    "**Original Content**: Java code for writing a file.\n",
    "**Updated Content**:  \n",
    "**Writing an HDFS File (Java)**:  \n",
    "```java\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.*;\n",
    "import java.io.IOException;\n",
    "\n",
    "public class HDFSWrite {\n",
    "    public static void main(String[] args) throws IOException {\n",
    "        Configuration conf = new Configuration();\n",
    "        FileSystem fs = FileSystem.get(conf);\n",
    "        Path path = new Path(\"/user/apitest.txt\");\n",
    "        if (!fs.exists(path)) {\n",
    "            FSDataOutputStream out = fs.create(path);\n",
    "            out.writeUTF(\"Hello, Big Data World!\");\n",
    "            out.close();\n",
    "        }\n",
    "        fs.close();\n",
    "    }\n",
    "}\n",
    "```\n",
    "**Purpose**: Creates and writes a string to a file in HDFS.\n",
    "\n",
    "**Updates**: Updated code for clarity and Hadoop 3.x compatibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 26: Compiling and Running HDFS Programs\n",
    "**Original Content**: Compilation and execution commands with errors.\n",
    "**Updated Content**:  \n",
    "**Compiling and Running HDFS Programs**:  \n",
    "1. **Compile**:  \n",
    "   ```bash\n",
    "   javac -cp $(hadoop classpath) HDFS*.java\n",
    "   ```\n",
    "2. **Package**:  \n",
    "   ```bash\n",
    "   jar cfe HDFSApp.jar HDFSWrite HDFS*.class\n",
    "   ```\n",
    "3. **Run**:  \n",
    "   ```bash\n",
    "   hadoop jar HDFSApp.jar HDFSWrite\n",
    "   ```\n",
    "**Note**: Use Hadoop 3.x classpath and ensure Java 11+ compatibility in 2025.\n",
    "\n",
    "**Updates**: Corrected commands, updated for modern Hadoop and Java versions.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 27: Introduction to MapReduce\n",
    "**Original Content**: Title \"Algorithmes & MapReduce\".\n",
    "**Updated Content**:  \n",
    "**MapReduce Overview**:  \n",
    "- **Purpose**: Framework for parallel processing of large datasets.  \n",
    "- **Components**:  \n",
    "  - **Map**: Processes input data into key-value pairs.  \n",
    "  - **Reduce**: Aggregates mapped data to produce final results.  \n",
    "- **Relevance in 2025**: Complements Spark for batch processing in Hadoop ecosystems.\n",
    "\n",
    "**Updates**: Added context and clarified MapReduce’s role alongside Spark.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 28: MapReduce Principles\n",
    "**Original Content**: Describes MapReduce with sales examples.\n",
    "**Updated Content**:  \n",
    "**MapReduce Principles**:  \n",
    "- **Goal**: Extract insights from large datasets.  \n",
    "- **Examples**:  \n",
    "  - Total sales of a product.  \n",
    "  - Most expensive item.  \n",
    "  - Average price.  \n",
    "- **Process**:  \n",
    "  - **Map**: Extracts relevant data per record.  \n",
    "  - **Reduce**: Aggregates mapped data (e.g., sum, max, average).  \n",
    "This approach scales to petabytes via distributed computing.\n",
    "\n",
    "**Updates**: Simplified examples and emphasized scalability.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 29: MapReduce Example\n",
    "**Original Content**: Table of car sales data.\n",
    "**Updated Content**:  \n",
    "**MapReduce Example: Car Sales**:  \n",
    "Input data (car sales):  \n",
    "\n",
    "| ID | Brand    | Model | Price  |\n",
    "|----|----------|-------|--------|\n",
    "| 1  | Renault  | Clio  | 4200   |\n",
    "| 2  | Fiat     | 500   | 8840   |\n",
    "| 3  | Peugeot  | 206   | 4300   |\n",
    "| 4  | Peugeot  | 306   | 6140   |\n",
    "\n",
    "**Task**: Calculate the maximum price.  \n",
    "- **Map**: Extract price from each record.  \n",
    "- **Reduce**: Find the maximum price (8840).\n",
    "\n",
    "**Updates**: Clarified task and process, corrected table formatting.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 30: MapReduce Example (Continued)\n",
    "**Original Content**: Explains Map and Reduce functions.\n",
    "**Updated Content**:  \n",
    "**MapReduce Workflow**:  \n",
    "- **Map Function**: Extracts price from each car record.  \n",
    "  ```python\n",
    "  def getPrice(car):\n",
    "      return car[\"price\"]\n",
    "  ```\n",
    "- **Reduce Function**: Computes maximum price.  \n",
    "  ```python\n",
    "  def maxPrice(prices):\n",
    "      return max(prices)\n",
    "  ```\n",
    "- **Output**: `max(map(getPrice, data))` yields 8840.  \n",
    "This logic is parallelizable across clusters.\n",
    "\n",
    "**Updates**: Provided clear Python pseudocode, emphasized parallelization.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 31: MapReduce in Python\n",
    "**Original Content**: Python code for mapping prices.\n",
    "**Updated Content**:  \n",
    "**Python MapReduce Example**:  \n",
    "```python\n",
    "data = [\n",
    "    {\"id\": 1, \"brand\": \"Renault\", \"model\": \"Clio\", \"price\": 4200},\n",
    "    {\"id\": 2, \"brand\": \"Fiat\", \"model\": \"500\", \"price\": 8840},\n",
    "    {\"id\": 3, \"brand\": \"Peugeot\", \"model\": \"206\", \"price\": 4300},\n",
    "    {\"id\": 4, \"brand\": \"Peugeot\", \"model\": \"306\", \"price\": 6140}\n",
    "]\n",
    "\n",
    "# Map: Extract prices\n",
    "prices = list(map(lambda car: car[\"price\"], data))\n",
    "print(prices)  # [4200, 8840, 4300, 6140]\n",
    "\n",
    "# Reduce: Find maximum\n",
    "max_price = max(prices)\n",
    "print(max_price)  # 8840\n",
    "```\n",
    "**Purpose**: Demonstrates MapReduce in Python 3.\n",
    "\n",
    "**Updates**: Updated to Python 3 syntax, used lambda for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 32: MapReduce Parallelization\n",
    "**Original Content**: Explains map and reduce parallelization.\n",
    "**Updated Content**:  \n",
    "**Parallelizing MapReduce**:  \n",
    "- **Map**: Fully parallelizable, as each record is processed independently.  \n",
    "  ```python\n",
    "  prices = [getPrice(car) for car in data]  # Can run on multiple nodes\n",
    "  ```\n",
    "- **Reduce**: Partially parallelizable in a hierarchical structure:  \n",
    "  1. Compute intermediate results for value pairs.  \n",
    "  2. Aggregate intermediates to final result.  \n",
    "In 2025, frameworks like Spark optimize this further.\n",
    "\n",
    "**Updates**: Simplified explanation, added Spark reference.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 33: YARN Overview\n",
    "**Original Content**: Introduces YARN as a resource manager.\n",
    "**Updated Content**:  \n",
    "**YARN (Yet Another Resource Negotiator)**:  \n",
    "- **Role**: Manages resources and schedules jobs in Hadoop clusters.  \n",
    "- **Features**:  \n",
    "  - Launches and monitors MapReduce or Spark jobs.  \n",
    "  - Handles resource allocation and fault tolerance.  \n",
    "  - Transparent to users, optimizing job execution.  \n",
    "- **Relevance in 2025**: Integrates with Kubernetes for hybrid cloud deployments.\n",
    "\n",
    "**Updates**: Added Spark and Kubernetes integration, clarified YARN’s role.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 34: MapReduce Framework\n",
    "**Original Content**: Describes MapReduce as a Java environment.\n",
    "**Updated Content**:  \n",
    "**MapReduce Framework**:  \n",
    "- **Purpose**: Java-based model for distributed data processing.  \n",
    "- **Components**:  \n",
    "  - **Mapper**: Processes input into key-value pairs.  \n",
    "  - **Reducer**: Aggregates mapped pairs.  \n",
    "- **Challenges**: Complex Java APIs; Spark is often preferred in 2025 for simplicity.  \n",
    "- **Use Case**: Batch processing in Hadoop ecosystems.\n",
    "\n",
    "**Updates**: Highlighted Spark’s prominence and simplified description.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 35: Key-Value Pairs in MapReduce\n",
    "**Original Content**: Introduces key-value pairs.\n",
    "**Updated Content**:  \n",
    "**Key-Value Pairs**:  \n",
    "- **Format**: Data is processed as (key, value) pairs (e.g., (line_number, text), (date, temperature)).  \n",
    "- **Role**: Enables flexible data processing in MapReduce.  \n",
    "- **Example**: A text file is split into (offset, line) pairs for mapping.  \n",
    "This abstraction supports scalability across clusters.\n",
    "\n",
    "**Updates**: Clarified concept with modern examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 36: Map Function Details\n",
    "**Original Content**: Describes Map function behavior.\n",
    "**Updated Content**:  \n",
    "**Map Function**:  \n",
    "- **Input**: A (key, value) pair (e.g., (offset, line)).  \n",
    "- **Output**: Zero or more (key, value) pairs.  \n",
    "- **Parallelization**: Each Map task runs independently on a data block.  \n",
    "- **Example**: Extract phone call durations from (offset, call_record) pairs.  \n",
    "YARN manages task distribution across nodes.\n",
    "\n",
    "**Updates**: Streamlined explanation, added example.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 37: Reduce Function Details\n",
    "**Original Content**: Describes Reduce function behavior.\n",
    "**Updated Content**:  \n",
    "**Reduce Function**:  \n",
    "- **Input**: A key and a list of values from Map tasks.  \n",
    "- **Output**: Typically one (key, value) pair per key.  \n",
    "- **Parallelization**: YARN groups pairs by key, enabling hierarchical reduction.  \n",
    "- **Example**: Sum call durations for a subscriber ID.  \n",
    "Critical for aggregating large datasets efficiently.\n",
    "\n",
    "**Updates**: Clarified process and added example.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 38: MapReduce Job Phases\n",
    "**Original Content**: Lists MapReduce phases (preprocessing, split, map, shuffle, reduce).\n",
    "**Updated Content**:  \n",
    "**MapReduce Job Stages**:  \n",
    "1. **Preprocessing**: Decompresses input files.  \n",
    "2. **Split**: Divides data into (key, value) pairs (e.g., (line_number, line)).  \n",
    "3. **Map**: Applies user-defined function to each pair.  \n",
    "4. **Shuffle & Sort**: Groups pairs by key for Reduce tasks.  \n",
    "5. **Reduce**: Aggregates values for each key.  \n",
    "YARN orchestrates these stages across clusters.\n",
    "\n",
    "**Updates**: Simplified and clarified stages.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 39: MapReduce Job Execution\n",
    "**Original Content**: Describes YARN’s role in job execution.\n",
    "**Updated Content**:  \n",
    "**MapReduce Execution Flow**:  \n",
    "1. **Data Location**: YARN queries NameNode for block locations.  \n",
    "2. **Split**: Data is split into (key, value) pairs.  \n",
    "3. **Map Tasks**: YARN launches Mappers on DataNodes.  \n",
    "4. **Shuffle**: Sorts and redistributes pairs by key.  \n",
    "5. **Reduce Tasks**: Aggregates results.  \n",
    "In 2025, Spark often replaces MapReduce for faster execution.\n",
    "\n",
    "**Updates**: Added Spark comparison, streamlined flow.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 40: Java API for MapReduce\n",
    "**Original Content**: Title only.\n",
    "**Updated Content**:  \n",
    "**MapReduce Java API**:  \n",
    "- **Purpose**: Develop distributed applications for Hadoop.  \n",
    "- **Key Classes**: `Mapper`, `Reducer`, `Job`.  \n",
    "- **Use Case**: Process large datasets with custom logic.  \n",
    "Examples follow, updated for Hadoop 3.x and Java 11+.\n",
    "\n",
    "**Updates**: Completed slide with API overview.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 41: Mapper Class\n",
    "**Original Content**: Java code for Mapper subclass.\n",
    "**Updated Content**:  \n",
    "**Mapper Class Example**:  \n",
    "```java\n",
    "import org.apache.hadoop.io.*;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import java.io.IOException;\n",
    "\n",
    "public class TraitementMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    @Override\n",
    "    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "        String line = value.toString();\n",
    "        Text outKey = new Text(line.split(\",\")[0]); // Example: Extract first column\n",
    "        IntWritable outValue = new IntWritable(1);\n",
    "        context.write(outKey, outValue);\n",
    "    }\n",
    "}\n",
    "```\n",
    "**Purpose**: Processes input lines into key-value pairs.\n",
    "\n",
    "**Updates**: Updated for Hadoop 3.x, added example logic.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 42: Reducer Class\n",
    "**Original Content**: Java code for Reducer subclass.\n",
    "**Updated Content**:  \n",
    "**Reducer Class Example**:  \n",
    "```java\n",
    "import org.apache.hadoop.io.*;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import java.io.IOException;\n",
    "\n",
    "public class TraitementReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    @Override\n",
    "    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "        int sum = 0;\n",
    "        for (IntWritable val : values) {\n",
    "            sum += val.get();\n",
    "        }\n",
    "        context.write(key, new IntWritable(sum));\n",
    "    }\n",
    "}\n",
    "```\n",
    "**Purpose**: Aggregates values for each key (e.g., summing counts).\n",
    "\n",
    "**Updates**: Updated for Hadoop 3.x, clarified logic.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 43: Driver Class\n",
    "**Original Content**: Java code for MapReduce driver.\n",
    "**Updated Content**:  \n",
    "**Driver Class Example**:  \n",
    "```java\n",
    "import org.apache.hadoop.conf.*;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.*;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class TraitementDriver extends Configured implements Tool {\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        System.exit(ToolRunner.run(new TraitementDriver(), args));\n",
    "    }\n",
    "\n",
    "    @Override\n",
    "    public int run(String[] args) throws Exception {\n",
    "        Configuration conf = getConf();\n",
    "        Job job = Job.getInstance(conf, \"Traitement\");\n",
    "        job.setJarByClass(TraitementDriver.class);\n",
    "        job.setMapperClass(TraitementMapper.class);\n",
    "        job.setReducerClass(TraitementReducer.class);\n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(IntWritable.class);\n",
    "        FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "        return job.waitForCompletion(true) ? 0 : 1;\n",
    "    }\n",
    "}\n",
    "```\n",
    "**Purpose**: Configures and runs a MapReduce job.\n",
    "\n",
    "**Updates**: Updated for Hadoop 3.x, added output types.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 44: Compiling and Running MapReduce\n",
    "**Original Content**: Compilation and execution commands.\n",
    "**Updated Content**:  \n",
    "**Running a MapReduce Job**:  \n",
    "1. **Compile**:  \n",
    "   ```bash\n",
    "   javac -cp $(hadoop classpath) Traitement*.java\n",
    "   ```\n",
    "2. **Package**:  \n",
    "   ```bash\n",
    "   jar cfe Traitement.jar TraitementDriver Traitement*.class\n",
    "   ```\n",
    "3. **Prepare**:  \n",
    "   ```bash\n",
    "   hdfs dfs -rm -r /user/output\n",
    "   ```\n",
    "4. **Run**:  \n",
    "   ```bash\n",
    "   yarn jar Traitement.jar TraitementDriver /user/input /user/output\n",
    "   ```\n",
    "**Note**: Use Java 11+ and Hadoop 3.x in 2025.\n",
    "\n",
    "**Updates**: Corrected commands, updated for modern Hadoop.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 45: MapReduce in Practice\n",
    "**Original Content**: Placeholder \"Un schema\".\n",
    "**Updated Content**:  \n",
    "**MapReduce Workflow Diagram**:  \n",
    "- **Input**: Data split into blocks.  \n",
    "- **Map**: Processes blocks into (key, value) pairs.  \n",
    "- **Shuffle**: Groups pairs by key.  \n",
    "- **Reduce**: Aggregates values per key.  \n",
    "- **Output**: Final results stored in HDFS.  \n",
    "This process is optimized by YARN for scalability.\n",
    "\n",
    "**Updates**: Completed slide with a workflow description.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 46: MapReduce Example (Phone Calls)\n",
    "**Original Content**: Phone call duration example.\n",
    "**Updated Content**:  \n",
    "**Example: Total Call Duration**:  \n",
    "- **Input**: CSV file of calls (subscriber_id, called_number, date, duration).  \n",
    "- **Map**: Emits (subscriber_id, duration) pairs.  \n",
    "- **Reduce**: Sums durations per subscriber_id.  \n",
    "- **Output**: Total call duration per subscriber.  \n",
    "YARN distributes tasks across nodes for efficiency.\n",
    "\n",
    "**Updates**: Clarified example and process.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 47: MapReduce Optimizations\n",
    "**Original Content**: Notes on Map and Reduce instances.\n",
    "**Updated Content**:  \n",
    "**MapReduce Optimizations**:  \n",
    "- **Map**: YARN runs one Mapper per node, processing multiple records sequentially.  \n",
    "- **Reduce**: Hierarchical reduction for large datasets, with multiple Reducers.  \n",
    "- **2025 Trends**: Spark often replaces MapReduce for faster, in-memory processing.  \n",
    "These optimizations ensure scalability for massive datasets.\n",
    "\n",
    "**Updates**: Added Spark comparison, simplified explanation.\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 48: Conclusion\n",
    "**Original Content**: Placeholder \"Dnnate\".\n",
    "**Updated Content**:  \n",
    "**Course Summary**:  \n",
    "- **Big Data**: Handles massive, complex datasets.  \n",
    "- **HDFS**: Scalable, fault-tolerant storage.  \n",
    "- **MapReduce**: Parallel processing framework.  \n",
    "- **Next Steps**: Explore Spark, cloud-native tools, and real-world applications in upcoming weeks.  \n",
    "This course prepares you for 2025’s data-driven world.\n",
    "\n",
    "**Updates**: Completed slide with a course wrap-up.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582e3be",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
